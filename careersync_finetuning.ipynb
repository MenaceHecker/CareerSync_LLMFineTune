{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fb926cd",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# CareerSync Fine-tuning - Load Dataset\n",
    "import json\n",
    "import os\n",
    "from google.colab import files\n",
    "\n",
    "# Load the CareerSync dataset\n",
    "file = json.load(open(\"careersync_dataset.json\", \"r\"))\n",
    "print(\"Sample data:\")\n",
    "print(json.dumps(file[0], indent=2))\n",
    "print(f\"\\nTotal samples: {len(file)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f123a455",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# CareerSync Fine-tuning - Install Dependencies\n",
    "!pip uninstall -y unsloth peft\n",
    "\n",
    "!pip install unsloth trl peft accelerate bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2168697",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# For GPU check\n",
    "import torch\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "print(f\"GPU: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'None'}\")\n",
    "\n",
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "\n",
    "model_name = \"unsloth/Phi-3-mini-4k-instruct-bnb-4bit\"\n",
    "max_seq_length = 2048  # Choose sequence length\n",
    "dtype = None  # Auto detection\n",
    "\n",
    "# Load model and tokenizer\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=model_name,\n",
    "    max_seq_length=max_seq_length,\n",
    "    dtype=dtype,\n",
    "    load_in_4bit=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f60bb237",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "def format_prompt(example):\n",
    "    # Format specifically for resume vs job description analysis\n",
    "    return f\"### Input: {example['input']}\\n### Output: {json.dumps(example['output'])}<|endoftext|>\"\n",
    "\n",
    "# Format the data\n",
    "formatted_data = [format_prompt(item) for item in file]\n",
    "dataset = Dataset.from_dict({\"text\": formatted_data})\n",
    "\n",
    "print(f\"Formatted dataset size: {len(dataset)}\")\n",
    "print(\"Sample formatted prompt:\")\n",
    "print(formatted_data[0][:500] + \"...\" if len(formatted_data[0]) > 500 else formatted_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5de6975",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Add LoRA adapters\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r=64,  # LoRA rank - higher = more capacity, more memory\n",
    "    target_modules=[\n",
    "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "        \"gate_proj\", \"up_proj\", \"down_proj\",\n",
    "    ],\n",
    "    lora_alpha=128,  # LoRA scaling factor (usually 2x rank)\n",
    "    lora_dropout=0,  # Supports any, but = 0 is optimized\n",
    "    bias=\"none\",     # Supports any, but = \"none\" is optimized\n",
    "    use_gradient_checkpointing=\"unsloth\",  # Unsloth's optimized version\n",
    "    random_state=3407,\n",
    "    use_rslora=False,  # Rank stabilized LoRA\n",
    "    loftq_config=None, # LoftQ\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a63bcca1",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "# Training arguments optimized for CareerSync dataset\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=dataset,\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=max_seq_length,\n",
    "    dataset_num_proc=2,\n",
    "    args=TrainingArguments(\n",
    "        per_device_train_batch_size=2,\n",
    "        gradient_accumulation_steps=4,  # Effective batch size = 8\n",
    "        warmup_steps=10,\n",
    "        num_train_epochs=5,  # Increased for better learning with small dataset\n",
    "        learning_rate=2e-4,\n",
    "        fp16=not torch.cuda.is_bf16_supported(),\n",
    "        bf16=torch.cuda.is_bf16_supported(),\n",
    "        logging_steps=5,  # More frequent logging for small dataset\n",
    "        optim=\"adamw_8bit\",\n",
    "        weight_decay=0.01,\n",
    "        lr_scheduler_type=\"linear\",\n",
    "        seed=3407,\n",
    "        output_dir=\"outputs\",\n",
    "        save_strategy=\"epoch\",\n",
    "        save_total_limit=2,\n",
    "        dataloader_pin_memory=False,\n",
    "        report_to=None,  # Disable wandb logging\n",
    "    ),\n",
    ")\n",
    "\n",
    "print(\"Starting training...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aa18f14",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Training arguments optimized for CareerSync dataset\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=dataset,\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=max_seq_length,\n",
    "    dataset_num_proc=2,\n",
    "    args=TrainingArguments(\n",
    "        per_device_train_batch_size=2,\n",
    "        gradient_accumulation_steps=4,  # Effective batch size = 8\n",
    "        warmup_steps=10,\n",
    "        num_train_epochs=5,  # Increased for better learning with small dataset\n",
    "        learning_rate=2e-4,\n",
    "        fp16=not torch.cuda.is_bf16_supported(),\n",
    "        bf16=torch.cuda.is_bf16_supported(),\n",
    "        logging_steps=5,  # More frequent logging for small dataset\n",
    "        optim=\"adamw_8bit\",\n",
    "        weight_decay=0.01,\n",
    "        lr_scheduler_type=\"linear\",\n",
    "        seed=3407,\n",
    "        output_dir=\"outputs\",\n",
    "        save_strategy=\"epoch\",\n",
    "        save_total_limit=2,\n",
    "        dataloader_pin_memory=False,\n",
    "        report_to=None,  # Disable wandb logging\n",
    "    ),\n",
    ")\n",
    "\n",
    "print(\"Starting training...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31ddf75d",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Train the model\n",
    "trainer_stats = trainer.train()\n",
    "print(\"Training completed!\")\n",
    "\n",
    "# Test the fine-tuned model\n",
    "FastLanguageModel.for_inference(model) # Enable native 2x faster inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4979f379",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Test with CareerSync-specific prompt\n",
    "test_input = \"\"\"Analyze resume vs job description:\n",
    "RESUME: Python Developer with 2 years experience, Django, Flask, PostgreSQL, Git. Built REST APIs and web applications.\n",
    "JOB: Backend Developer position. Python, FastAPI, microservices, 3+ years experience, AWS knowledge preferred.\"\"\"\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": test_input},\n",
    "]\n",
    "\n",
    "inputs = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=True,\n",
    "    add_generation_prompt=True,\n",
    "    return_tensors=\"pt\",\n",
    ").to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c953e15f",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Generate response\n",
    "outputs = model.generate(\n",
    "    input_ids=inputs,\n",
    "    max_new_tokens=512,  # Increased for detailed analysis\n",
    "    use_cache=True,\n",
    "    temperature=0.3,  # Lower temperature for more consistent analysis\n",
    "    do_sample=True,\n",
    "    top_p=0.9,\n",
    "    pad_token_id=tokenizer.eos_token_id,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee5896cd",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Decode and print\n",
    "response = tokenizer.batch_decode(outputs)[0]\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"MODEL RESPONSE:\")\n",
    "print(\"=\"*50)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7bca901",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Test with another example\n",
    "test_input_2 = \"\"\"Analyze resume vs job description:\n",
    "RESUME: Marketing Coordinator, 1 year experience, social media management, content creation, Google Analytics, Canva.\n",
    "JOB: Digital Marketing Manager, 4+ years experience, SEO, PPC campaigns, marketing automation, team leadership.\"\"\"\n",
    "\n",
    "messages_2 = [\n",
    "    {\"role\": \"user\", \"content\": test_input_2},\n",
    "]\n",
    "\n",
    "inputs_2 = tokenizer.apply_chat_template(\n",
    "    messages_2,\n",
    "    tokenize=True,\n",
    "    add_generation_prompt=True,\n",
    "    return_tensors=\"pt\",\n",
    ").to(\"cuda\")\n",
    "\n",
    "outputs_2 = model.generate(\n",
    "    input_ids=inputs_2,\n",
    "    max_new_tokens=512,\n",
    "    use_cache=True,\n",
    "    temperature=0.3,\n",
    "    do_sample=True,\n",
    "    top_p=0.9,\n",
    "    pad_token_id=tokenizer.eos_token_id,\n",
    ")\n",
    "\n",
    "response_2 = tokenizer.batch_decode(outputs_2)[0]\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"MODEL RESPONSE 2:\")\n",
    "print(\"=\"*50)\n",
    "print(response_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4658fa74",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Save the model in GGUF format for deployment\n",
    "print(\"Saving model in GGUF format...\")\n",
    "model.save_pretrained_gguf(\"careersync_model\", tokenizer, quantization_method=\"q4_k_m\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f064dc52",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Download the model file\n",
    "gguf_files = [f for f in os.listdir(\"careersync_model\") if f.endswith(\".gguf\")]\n",
    "if gguf_files:\n",
    "    gguf_file = os.path.join(\"careersync_model\", gguf_files[0])\n",
    "    print(f\"Downloading: {gguf_file}\")\n",
    "    files.download(gguf_file)\n",
    "else:\n",
    "    print(\"No GGUF files found!\")\n",
    "\n",
    "print(\"\\nTraining completed! Your CareerSync model is ready for deployment.\")\n",
    "print(\"The model can now analyze resumes against job descriptions and provide:\")\n",
    "print(\"- Match scores\")\n",
    "print(\"- Matched skills\")\n",
    "print(\"- Missing skills\") \n",
    "print(\"- Recommendations\")\n",
    "print(\"- Role alignment assessment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3be50e45",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
